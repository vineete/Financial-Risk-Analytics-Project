{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.read_csv(\"probabilities.csv\")\n",
    "loss = pd.read_csv(\"t1_t5_loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.head())\n",
    "print(loss.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1_yr = loss[loss[\"T_i\"] == 2008]\n",
    "loss_5_yr = loss[loss[\"T_i\"] == 2012]\n",
    "loss_1 = loss_1_yr[[\"LoanNum\",\"Loss\"]]\n",
    "loss_5 = loss_5_yr[[\"LoanNum\",\"Loss\"]]\n",
    "print(loss_1.head())\n",
    "print(loss_5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = probs[[\"X1\",\"X2\",\"X3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = pd.read_excel(\"dataMoreFeatures.xlsx\")\n",
    "actual = actual[[\"LoanNum\",\"GrossApproval\"]]\n",
    "print(actual.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = probs.merge(actual,left_on = \"X1\",right_on = \"LoanNum\")\n",
    "total_loan_amount = np.sum(val_counts[\"GrossApproval\"])\n",
    "print(total_loan_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = probs.merge(loss_1,left_on = \"X1\",right_on = \"LoanNum\")\n",
    "new2 = new.merge(loss_5,left_on = \"X1\",right_on = \"LoanNum\")\n",
    "final = new2[[\"X2\",\"X3\",\"Loss_x\",\"Loss_y\"]]\n",
    "print(final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 10000\n",
    "total_loss = []\n",
    "length = len(final)\n",
    "for i in range(num_simulations):\n",
    "    vec = np.random.uniform(low = 0, high = 1, size = (1,length))\n",
    "    vec = np.array(vec).flatten()\n",
    "    loss_vals = final[final[\"X3\"] < vec][\"Loss_x\"]\n",
    "    loss_vals = np.array(loss_vals)\n",
    "    total_loss.append(loss_vals.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = np.array(total_loss)\n",
    "print(len(total_loss))\n",
    "print(np.mean(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CDF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "X1 = np.sort(total_loss)\n",
    "Y1 = np.array(range(len(total_loss)))/float(len(total_loss))\n",
    "plt.plot(X1,Y1)\n",
    "plt.xlabel(\"Loss Amount\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "#plt.savefig(\"CDF_of_Loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram\n",
    "n, bins, patches = plt.hist(x=total_loss, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "#plt.savefig(\"loss_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram\n",
    "\n",
    "q = total_loss/total_loan_amount\n",
    "n, bins, patches = plt.hist(x=q, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "#plt.savefig(\"tranche_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_losses = np.sort(total_loss)\n",
    "first_ind = int(0.95*10000)\n",
    "second_ind = int(0.99*10000)\n",
    "first_loss = sorted_losses[first_ind]\n",
    "second_loss = sorted_losses[second_ind]\n",
    "print(\"95% VAR: \", first_loss)\n",
    "print(\"99% VAR: \", second_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average VAR\n",
    "average_first_loss = np.mean(sorted_losses[first_ind:])\n",
    "average_second_loss = np.mean(sorted_losses[second_ind:])\n",
    "average_first_std = np.std(sorted_losses[first_ind:])\n",
    "average_second_std = np.std(sorted_losses[second_ind:])\n",
    "print(\"95% Average VAR\", average_first_loss)\n",
    "print(\"99% Average VAR\", average_second_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate, lower, upper = mean_confidence_interval(sorted_losses[first_ind:])\n",
    "estimate2, lower2, upper2 = mean_confidence_interval(sorted_losses[second_ind:])\n",
    "print(\"Estimate, lower and upper for 95% AVAR\", (estimate,lower,upper))\n",
    "print(\"Estimate, lower and upper for 99% AVAR\", (estimate2,lower2,upper2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
